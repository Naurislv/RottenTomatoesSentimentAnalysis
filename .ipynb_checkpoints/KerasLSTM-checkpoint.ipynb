{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import cv2\n",
    "\n",
    "# Dependecy imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import SpatialDropout1D, Dropout\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale\n",
    "from tqdm import tqdm # Progress bar\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# Local imports\n",
    "from data.load_dataset import TRAIN_SET, TEST_SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples:\n",
      "Token index for [story] 40\n",
      "Token index for [comedy] 60\n",
      "Token index for [movie] 17\n",
      "\n",
      "Examples:\n",
      "would have a hard time sitting through this one  --> [93, 35, 2, 198, 59, 1072, 96, 18, 28]\n",
      " trouble every day is a plodding mess  --> [942, 124, 329, 8, 2, 1917, 607]\n",
      "a source --> [2, 1214]\n",
      "\n",
      "Example\n",
      "[93, 35, 2, 198, 59, 1072, 96, 18, 28] --> [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0   93   35    2  198   59 1072   96   18   28]\n",
      "\n",
      "Input train data shape: (156060, 45)\n",
      "Input test data shape: (66292, 46)\n"
     ]
    }
   ],
   "source": [
    "max_fatures = 2000 # Top 2000 words\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "\n",
    "# The training phase is by means of the fit_on_texts method and you\n",
    "# can see the word index using the word_index property:\n",
    "tokenizer.fit_on_texts(TRAIN_SET['Phrase'].values)\n",
    "\n",
    "print(\"\\nExamples:\")\n",
    "print('Token index for [story]', tokenizer.word_index['story'])\n",
    "print('Token index for [comedy]', tokenizer.word_index['comedy'])\n",
    "print('Token index for [movie]', tokenizer.word_index['movie'])\n",
    "\n",
    "# texts_to_sequences method turns input into numerical arrays\n",
    "train_data = tokenizer.texts_to_sequences(TRAIN_SET['Phrase'].values)\n",
    "test_data = tokenizer.texts_to_sequences(TEST_SET['Phrase'].values)\n",
    "\n",
    "print(\"\\nExamples:\")\n",
    "print(TRAIN_SET['Phrase'][100], '-->', train_data[100])\n",
    "print(TRAIN_SET['Phrase'][200], '-->', train_data[200])\n",
    "print(TRAIN_SET['Phrase'][300], '-->', train_data[300])\n",
    "\n",
    "# All Phrase numerical values reshape to match size for all\n",
    "train_data_pad = pad_sequences(train_data)\n",
    "test_data_pad = pad_sequences(test_data)\n",
    "print(\"\\nExample\")\n",
    "print(train_data[100], '-->', train_data_pad[100])\n",
    "\n",
    "print('\\nInput train data shape:', train_data_pad.shape)\n",
    "print('Input test data shape:', test_data_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train data into Train and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample labels:\n",
      "[[0 1 0 0 0]\n",
      " [0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# One Hot encoding\n",
    "train_labels = pd.get_dummies(TRAIN_SET['Sentiment']).values\n",
    "print('Sample labels:')\n",
    "print(train_labels[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Keras 1. attempt\n",
    "\n",
    "With embeddings __without Word2vec__ so there are no semantic similarity here in embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196 # Output Neurons\n",
    "batch_size = 128\n",
    "drop_out = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 45, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 45, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 985       \n",
      "=================================================================\n",
      "Total params: 511,785\n",
      "Trainable params: 511,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim, input_length=train_data_pad.shape[1]))\n",
    "model.add(SpatialDropout1D(drop_out))\n",
    "\n",
    "# LSTMs\n",
    "model.add(LSTM(lstm_out))\n",
    "model.add(Dropout(drop_out))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You don't have to be genius to spot here an overfitting\n",
    "\n",
    "No playing with Hyperparameters. Moving along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/5\n",
      "124848/124848 [==============================] - 47s - loss: 1.0682 - acc: 0.5794 - val_loss: 1.0605 - val_acc: 0.5822\n",
      "Epoch 2/5\n",
      "124848/124848 [==============================] - 44s - loss: 0.9611 - acc: 0.6225 - val_loss: 1.0457 - val_acc: 0.5878\n",
      "Epoch 3/5\n",
      "124848/124848 [==============================] - 45s - loss: 0.9320 - acc: 0.6338 - val_loss: 1.0576 - val_acc: 0.5856\n",
      "Epoch 4/5\n",
      "124848/124848 [==============================] - 45s - loss: 0.9077 - acc: 0.6428 - val_loss: 1.0636 - val_acc: 0.5869\n",
      "Epoch 5/5\n",
      "124848/124848 [==============================] - 44s - loss: 0.8895 - acc: 0.6499 - val_loss: 1.0660 - val_acc: 0.5900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f4aece48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data_pad, train_labels, epochs=5, batch_size=batch_size, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Keras 2. attempt\n",
    "\n",
    "With Word2vec embeddings. The idea is that instead of mapping sequences of integer numbers to sequences of floats happens in a way which preserves the semantic affinity. There are various pretrained word2vec datasets on the net, we will use Google Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Google Word2Vec pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin', binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Facebook', 0.7563532590866089),\n",
       " ('FaceBook', 0.7076998949050903),\n",
       " ('twitter', 0.6988551616668701),\n",
       " ('myspace', 0.6941817402839661),\n",
       " ('Twitter', 0.6642444729804993),\n",
       " ('Facebook.com', 0.6529868245124817),\n",
       " ('FacebookFacebook', 0.6162722110748291),\n",
       " ('facebook.com', 0.6135972142219543),\n",
       " ('Twitter.com', 0.6102107763290405),\n",
       " ('TwitterTwitter', 0.6085205078125)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.7203598022460938),\n",
       " ('pear', 0.6450696587562561),\n",
       " ('fruit', 0.6410146355628967),\n",
       " ('berry', 0.6302294731140137),\n",
       " ('pears', 0.6133961081504822),\n",
       " ('strawberry', 0.6058261394500732),\n",
       " ('peach', 0.6025873422622681),\n",
       " ('potato', 0.596093475818634),\n",
       " ('grape', 0.5935864448547363),\n",
       " ('blueberry', 0.5866668224334717)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple_AAPL', 0.7456985712051392),\n",
       " ('Apple_Nasdaq_AAPL', 0.7300410270690918),\n",
       " ('Apple_NASDAQ_AAPL', 0.7175089716911316),\n",
       " ('Apple_Computer', 0.7145973443984985),\n",
       " ('iPhone', 0.6924266219139099),\n",
       " ('Apple_NSDQ_AAPL', 0.6868604421615601),\n",
       " ('Steve_Jobs', 0.6758422255516052),\n",
       " ('iPad', 0.6580768823623657),\n",
       " ('Apple_nasdaq_AAPL', 0.6444970965385437),\n",
       " ('Apple_iPad', 0.622774600982666)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar('Apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 11385\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([row['Phrase'].split(' ') for _, row in TRAIN_SET.iterrows()])\n",
    "\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_vector(tokens, size):\n",
    "    # Given a list of phrase tokens, creates an averaged phrase vector.\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += word2vec_model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156060it [00:07, 19715.88it/s]\n"
     ]
    }
   ],
   "source": [
    "n_dim = word2vec_model.vector_size\n",
    "\n",
    "train_vecs_w2v = np.concatenate(\n",
    "    [build_word_vector(z, n_dim) for z in tqdm(map(lambda x: x.split(' '), list(TRAIN_SET['Phrase'].values)))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                9632      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 9,797\n",
      "Trainable params: 9,797\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=n_dim))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/10\n",
      "140454/140454 [==============================] - 9s - loss: 1.0032 - acc: 0.5889 - val_loss: 1.0280 - val_acc: 0.5823\n",
      "Epoch 2/10\n",
      "140454/140454 [==============================] - 8s - loss: 0.9372 - acc: 0.6184 - val_loss: 1.0254 - val_acc: 0.5873\n",
      "Epoch 3/10\n",
      "140454/140454 [==============================] - 8s - loss: 0.9184 - acc: 0.6270 - val_loss: 1.0287 - val_acc: 0.5814\n",
      "Epoch 4/10\n",
      "140454/140454 [==============================] - 8s - loss: 0.9061 - acc: 0.6309 - val_loss: 1.0313 - val_acc: 0.5832\n",
      "Epoch 5/10\n",
      "140454/140454 [==============================] - 8s - loss: 0.8973 - acc: 0.6351 - val_loss: 1.0323 - val_acc: 0.5877\n",
      "Epoch 6/10\n",
      " 12416/140454 [=>............................] - ETA: 7s - loss: 0.8749 - acc: 0.6428"
     ]
    }
   ],
   "source": [
    "model.fit(train_vecs_w2v, train_labels, epochs=10, batch_size=32, verbose=1,\n",
    "          validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs_w2v = np.concatenate(\n",
    "    [build_word_vector(z, n_dim) for z in tqdm(map(lambda x: x.split(' '), list(TEST_SET['Phrase'].values)))])\n",
    "test_vecs_w2v = scale(train_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.argmax(model.predict(test_vecs_w2v), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Phrase'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_SET['Preds'] = test_preds\n",
    "\n",
    "create_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sumbission(test_data):\n",
    "    \"\"\"Test data pandas input.\"\"\"\n",
    "    \n",
    "    for _, row in test_data.iterrows():\n",
    "        row['PhraseId']\n",
    "\n",
    "    fieldnames = ['PhraseId', 'Sentiment']\n",
    "\n",
    "    with open(path + 'labels.csv', 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow(p_lock_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".... In Progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

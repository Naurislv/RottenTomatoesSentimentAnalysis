{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Dependecy imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import SpatialDropout1D, Dropout\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale\n",
    "from tqdm import tqdm # Progress bar\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# Local imports\n",
    "from data.load_dataset import TRAIN_SET, TEST_SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples:\n",
      "Token index for [story] 40\n",
      "Token index for [comedy] 60\n",
      "Token index for [movie] 17\n",
      "\n",
      "Examples:\n",
      "would have a hard time sitting through this one  --> [93, 35, 2, 198, 59, 1072, 96, 18, 28]\n",
      " trouble every day is a plodding mess  --> [942, 124, 329, 8, 2, 1917, 607]\n",
      "a source --> [2, 1214]\n",
      "\n",
      "Example\n",
      "[93, 35, 2, 198, 59, 1072, 96, 18, 28] --> [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0   93   35    2  198   59 1072   96   18   28]\n",
      "\n",
      "Input train data shape: (156060, 45)\n",
      "Input test data shape: (66292, 46)\n"
     ]
    }
   ],
   "source": [
    "max_fatures = 2000 # Top 2000 words\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "\n",
    "# The training phase is by means of the fit_on_texts method and you\n",
    "# can see the word index using the word_index property:\n",
    "tokenizer.fit_on_texts(TRAIN_SET['Phrase'].values)\n",
    "\n",
    "print(\"\\nExamples:\")\n",
    "print('Token index for [story]', tokenizer.word_index['story'])\n",
    "print('Token index for [comedy]', tokenizer.word_index['comedy'])\n",
    "print('Token index for [movie]', tokenizer.word_index['movie'])\n",
    "\n",
    "# texts_to_sequences method turns input into numerical arrays\n",
    "train_data = tokenizer.texts_to_sequences(TRAIN_SET['Phrase'].values)\n",
    "test_data = tokenizer.texts_to_sequences(TEST_SET['Phrase'].values)\n",
    "\n",
    "print(\"\\nExamples:\")\n",
    "print(TRAIN_SET['Phrase'][100], '-->', train_data[100])\n",
    "print(TRAIN_SET['Phrase'][200], '-->', train_data[200])\n",
    "print(TRAIN_SET['Phrase'][300], '-->', train_data[300])\n",
    "\n",
    "# All Phrase numerical values reshape to match size for all\n",
    "train_data_pad = pad_sequences(train_data)\n",
    "test_data_pad = pad_sequences(test_data)\n",
    "print(\"\\nExample\")\n",
    "print(train_data[100], '-->', train_data_pad[100])\n",
    "\n",
    "print('\\nInput train data shape:', train_data_pad.shape)\n",
    "print('Input test data shape:', test_data_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train data into Train and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample labels:\n",
      "[[0 1 0 0 0]\n",
      " [0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# One Hot encoding\n",
    "train_labels = pd.get_dummies(TRAIN_SET['Sentiment']).values\n",
    "print('Sample labels:')\n",
    "print(train_labels[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Keras 1. attempt\n",
    "\n",
    "With embeddings __without Word2vec__ so there are no semantic similarity here in embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196 # Output Neurons\n",
    "batch_size = 128\n",
    "drop_out = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 45, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 45, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 985       \n",
      "=================================================================\n",
      "Total params: 511,785\n",
      "Trainable params: 511,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim, input_length=train_data_pad.shape[1]))\n",
    "model.add(SpatialDropout1D(drop_out))\n",
    "\n",
    "# LSTMs\n",
    "model.add(LSTM(lstm_out))\n",
    "model.add(Dropout(drop_out))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You don't have to be genius to spot here an overfitting\n",
    "\n",
    "No playing with Hyperparameters. Moving along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/5\n",
      "124848/124848 [==============================] - 47s - loss: 1.0682 - acc: 0.5794 - val_loss: 1.0605 - val_acc: 0.5822\n",
      "Epoch 2/5\n",
      "124848/124848 [==============================] - 44s - loss: 0.9611 - acc: 0.6225 - val_loss: 1.0457 - val_acc: 0.5878\n",
      "Epoch 3/5\n",
      "124848/124848 [==============================] - 45s - loss: 0.9320 - acc: 0.6338 - val_loss: 1.0576 - val_acc: 0.5856\n",
      "Epoch 4/5\n",
      "124848/124848 [==============================] - 45s - loss: 0.9077 - acc: 0.6428 - val_loss: 1.0636 - val_acc: 0.5869\n",
      "Epoch 5/5\n",
      "124848/124848 [==============================] - 44s - loss: 0.8895 - acc: 0.6499 - val_loss: 1.0660 - val_acc: 0.5900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f4aece48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data_pad, train_labels, epochs=5, batch_size=batch_size, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Keras 2. attempt\n",
    "\n",
    "With Word2vec embeddings. The idea is that instead of mapping sequences of integer numbers to sequences of floats happens in a way which preserves the semantic affinity. There are various pretrained word2vec datasets on the net, we will use Google Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Google Word2Vec pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin', binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Facebook', 0.7563532590866089),\n",
       " ('FaceBook', 0.7076998949050903),\n",
       " ('twitter', 0.6988551616668701),\n",
       " ('myspace', 0.6941817402839661),\n",
       " ('Twitter', 0.6642444729804993),\n",
       " ('Facebook.com', 0.6529868245124817),\n",
       " ('FacebookFacebook', 0.6162722110748291),\n",
       " ('facebook.com', 0.6135972142219543),\n",
       " ('Twitter.com', 0.6102107763290405),\n",
       " ('TwitterTwitter', 0.6085205078125)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.7203598022460938),\n",
       " ('pear', 0.6450696587562561),\n",
       " ('fruit', 0.6410146355628967),\n",
       " ('berry', 0.6302294731140137),\n",
       " ('pears', 0.6133961081504822),\n",
       " ('strawberry', 0.6058261394500732),\n",
       " ('peach', 0.6025873422622681),\n",
       " ('potato', 0.596093475818634),\n",
       " ('grape', 0.5935864448547363),\n",
       " ('blueberry', 0.5866668224334717)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple_AAPL', 0.7456985712051392),\n",
       " ('Apple_Nasdaq_AAPL', 0.7300410270690918),\n",
       " ('Apple_NASDAQ_AAPL', 0.7175089716911316),\n",
       " ('Apple_Computer', 0.7145973443984985),\n",
       " ('iPhone', 0.6924266219139099),\n",
       " ('Apple_NSDQ_AAPL', 0.6868604421615601),\n",
       " ('Steve_Jobs', 0.6758422255516052),\n",
       " ('iPad', 0.6580768823623657),\n",
       " ('Apple_nasdaq_AAPL', 0.6444970965385437),\n",
       " ('Apple_iPad', 0.622774600982666)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar('Apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 11385\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([row['Phrase'].split(' ') for _, row in TRAIN_SET.iterrows()])\n",
    "\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_vector(tokens, size):\n",
    "    # Given a list of phrase tokens, creates an averaged phrase vector.\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += word2vec_model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156060it [00:07, 19715.88it/s]\n"
     ]
    }
   ],
   "source": [
    "n_dim = word2vec_model.vector_size\n",
    "\n",
    "train_vecs_w2v = np.concatenate(\n",
    "    [build_word_vector(z, n_dim) for z in tqdm(map(lambda x: x.split(' '), list(TRAIN_SET['Phrase'].values)))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                19264     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 19,589\n",
      "Trainable params: 19,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=n_dim))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.9296 - acc: 0.6233     \n",
      "Epoch 2/20\n",
      "156060/156060 [==============================] - 8s - loss: 0.9050 - acc: 0.6328     \n",
      "Epoch 3/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8889 - acc: 0.6405     \n",
      "Epoch 4/20\n",
      "156060/156060 [==============================] - 8s - loss: 0.8760 - acc: 0.6432     \n",
      "Epoch 5/20\n",
      "156060/156060 [==============================] - 8s - loss: 0.8663 - acc: 0.6485     \n",
      "Epoch 6/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8587 - acc: 0.6521     \n",
      "Epoch 7/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8517 - acc: 0.6541     \n",
      "Epoch 8/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8459 - acc: 0.6564     \n",
      "Epoch 9/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8400 - acc: 0.6581     \n",
      "Epoch 10/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8356 - acc: 0.6603     \n",
      "Epoch 11/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8313 - acc: 0.6611     \n",
      "Epoch 12/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8275 - acc: 0.6641     \n",
      "Epoch 13/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8238 - acc: 0.6652     \n",
      "Epoch 14/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8208 - acc: 0.6668     \n",
      "Epoch 15/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8177 - acc: 0.6666     \n",
      "Epoch 16/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8155 - acc: 0.6692     \n",
      "Epoch 17/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8131 - acc: 0.6699     \n",
      "Epoch 18/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8109 - acc: 0.6704     \n",
      "Epoch 19/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8089 - acc: 0.6713     \n",
      "Epoch 20/20\n",
      "156060/156060 [==============================] - 9s - loss: 0.8068 - acc: 0.6712     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe46d0c0b38>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_vecs_w2v, train_labels, epochs=20, batch_size=32, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66292it [00:03, 20867.45it/s]\n"
     ]
    }
   ],
   "source": [
    "test_vecs_w2v = np.concatenate(\n",
    "    [build_word_vector(z, n_dim) for z in tqdm(map(lambda x: x.split(' '), list(TEST_SET['Phrase'].values)))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.argmax(model.predict(test_vecs_w2v), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET['Sentiment'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET[['PhraseId', 'Sentiment']].to_csv('data/submission.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".... In Progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
